{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "152a7faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n",
      "108611\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "## Import dataset\n",
    "dataset = ImageFolder(\"data/plantvillage dataset/combined\")\n",
    "\n",
    "## Display dataset classes and the length of the dataset\n",
    "print(dataset.classes)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c7588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "## Transforms the data that is used for training\n",
    "train_transform = transforms.Compose([\n",
    "    ## Resizes the image\n",
    "    transforms.Resize(256),\n",
    "    ## Randomly crops the image in order to increase the number of images that are used to train the model.\n",
    "    transforms.RandomCrop(224),\n",
    "    ## Randomly flips images horizontally in order to increase the number of images that are used to train the model.\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    ## Transforms the images used for training into tensors\n",
    "    transforms.ToTensor(),\n",
    "    ## Normalizes the RGB values based on the ImageNet means and standard deviations\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "## Transforms the data that is used for training\n",
    "val_transform = transforms.Compose([\n",
    "    ## Resizes the image\n",
    "    transforms.Resize(256),\n",
    "    ## Center crops the images that are used for validation\n",
    "    transforms.CenterCrop(224),\n",
    "    ## Transforms the images used to validation into tensors\n",
    "    transforms.ToTensor(),\n",
    "    ## Normalizes the RGB values based on the ImageNet means and standard deviations\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "## The test transforms are the same as the validation transforms.\n",
    "test_transform = val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3b3f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "## Creates the training, validation, and testing datasets\n",
    "train_dataset = ImageFolder(\"data/plantvillage dataset/combined\", transform=train_transform)\n",
    "val_dataset = ImageFolder(\"data/plantvillage dataset/combined\", transform=val_transform)\n",
    "test_dataset = ImageFolder(\"data/plantvillage dataset/combined\", transform=test_transform)\n",
    "\n",
    "## identify training, validation, and test sizes\n",
    "train_size = int(0.7 * len(train_dataset))\n",
    "val_size = int(0.15 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14184ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "\n",
    "## This is the seed that is used when training the model. We use this in order to get the same results across training sessions.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## This creates a random permutation of the indices based on the size indicated.\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "## This identifies the indices that are used for the training dataset\n",
    "train_indices = indices[:train_size]\n",
    "## This identifies the indices that are used for the validation dataset\n",
    "val_indices = indices[train_size:train_size+val_size]\n",
    "## This identifies the indices that are used for the testing dataset\n",
    "test_indices = indices[train_size+val_size:]\n",
    "\n",
    "## These create new training, validation, and testing datasets that are the sizes indicated by their corresponding indices\n",
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "val_dataset = Subset(val_dataset, val_indices)\n",
    "test_dataset = Subset(test_dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "094c2f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "## These create the loaders for the training, validation, and testing datasets\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f7bc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([32, 3, 224, 224])\n",
      "Image value range: -2.118 to 2.640\n",
      "Labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Batch shape: {images.shape}\")  # Should be [32, 3, 224, 224]\n",
    "print(f\"Image value range: {images.min():.3f} to {images.max():.3f}\")  # Should be around -2 to 2 (normalized)\n",
    "print(f\"Labels shape: {labels.shape}\")  # Should be [32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5c04392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d84e1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=38, bias=True)\n",
      ")\n",
      "model.fc: Linear(in_features=512, out_features=38, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "## This assigns the model to the resnet18 pretrained model\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "## The number of features is assigned to the number of in_features.\n",
    "num_ftrs = model.fc.in_features\n",
    "## The fully connected layer is assigned Linear module in the neural network module of torch. This makes the output size 38.\n",
    "model.fc = nn.Linear(num_ftrs, 38)\n",
    "print(f\"model: {model}\")\n",
    "print(f\"model.fc: {model.fc}\")\n",
    "\n",
    "## This moves the model to the device (CPU)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8eaf851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "## This identifies the loss function that is being used\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "## This identifies the optimizer that is used for the model.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a67769c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2376/2376 [01:49<00:00, 21.64it/s]\n",
      "Validation: 100%|██████████| 510/510 [00:20<00:00, 24.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3008 | Train Acc: 90.68%\n",
      "Val Loss:   0.1364 | Val Acc:   95.46%\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2376/2376 [01:48<00:00, 21.92it/s]\n",
      "Validation: 100%|██████████| 510/510 [00:20<00:00, 24.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1405 | Train Acc: 95.48%\n",
      "Val Loss:   0.0982 | Val Acc:   96.86%\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2376/2376 [01:48<00:00, 21.97it/s]\n",
      "Validation: 100%|██████████| 510/510 [00:20<00:00, 24.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1079 | Train Acc: 96.47%\n",
      "Val Loss:   0.0892 | Val Acc:   97.07%\n",
      "\n",
      "Epoch 4/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2376/2376 [01:47<00:00, 22.13it/s]\n",
      "Validation: 100%|██████████| 510/510 [00:20<00:00, 24.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0880 | Train Acc: 97.19%\n",
      "Val Loss:   0.1224 | Val Acc:   96.61%\n",
      "\n",
      "Epoch 5/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2376/2376 [01:47<00:00, 22.02it/s]\n",
      "Validation: 100%|██████████| 510/510 [00:21<00:00, 24.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0749 | Train Acc: 97.57%\n",
      "Val Loss:   0.0453 | Val Acc:   98.51%\n",
      "\n",
      "Epoch 6/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2376/2376 [01:47<00:00, 22.11it/s]\n",
      "Validation: 100%|██████████| 510/510 [00:20<00:00, 25.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0614 | Train Acc: 98.04%\n",
      "Val Loss:   0.0623 | Val Acc:   98.18%\n",
      "\n",
      "Epoch 7/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2376/2376 [01:47<00:00, 22.10it/s]\n",
      "Validation: 100%|██████████| 510/510 [00:22<00:00, 22.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0526 | Train Acc: 98.30%\n",
      "Val Loss:   0.0366 | Val Acc:   98.82%\n",
      "\n",
      "Epoch 8/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2376/2376 [01:48<00:00, 21.85it/s]\n",
      "Validation: 100%|██████████| 510/510 [00:21<00:00, 23.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0502 | Train Acc: 98.36%\n",
      "Val Loss:   0.0400 | Val Acc:   98.67%\n",
      "\n",
      "Epoch 9/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2376/2376 [01:50<00:00, 21.43it/s]\n",
      "Validation: 100%|██████████| 510/510 [00:21<00:00, 23.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0445 | Train Acc: 98.61%\n",
      "Val Loss:   0.0326 | Val Acc:   98.90%\n",
      "\n",
      "Epoch 10/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2376/2376 [01:50<00:00, 21.41it/s]\n",
      "Validation: 100%|██████████| 510/510 [00:20<00:00, 24.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0400 | Train Acc: 98.71%\n",
      "Val Loss:   0.0547 | Val Acc:   98.43%\n",
      "\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "## This identifies the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "## This loop trains the model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  ## Disable gradient calculation\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            ## Move data to CPU\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            ## Forward pass only (no backward pass needed)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            ## Track statistics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    ## Calculate epoch statistics\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    ## -------------------- PRINT RESULTS --------------------\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n",
    "    print(f\"Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af0eb962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"plant_disease_model.pth\")\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a174f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 510/510 [00:21<00:00, 23.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 98.64%\n",
      "Correctly classified 16071 out of 16293 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        test_total += labels.size(0)\n",
    "        test_correct +=  (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"Correctly classified {test_correct} out of {test_total} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82f189d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def predict_image(image_path, model, transform, class_names, device):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "\n",
    "    predicted_class = class_names[predicted_idx.item()]\n",
    "    confidence_percent = confidence.item() * 100\n",
    "\n",
    "    return predicted_class, confidence_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9c0f65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Tomato___Early_blight\n",
      "Confidence: 99.89%\n"
     ]
    }
   ],
   "source": [
    "prediction, confidence = predict_image(\"./data/plantvillage dataset/combined/Tomato___Early_blight/004cbe60-8ff9-4965-92df-e86694d5e9ba___RS_Erly.B 8253_final_masked.jpg\", model, val_transform, dataset.classes, device)\n",
    "\n",
    "print(f\"Predicted: {prediction}\")\n",
    "print(f\"Confidence: {confidence:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
